{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project NLP Boulo Loisel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14L3yS7bHkCC",
        "colab_type": "text"
      },
      "source": [
        "# NLP Project: Prediction of product success on Amazon\n",
        "### ENSAE Paris\n",
        "### Authors: Yann Boulo and Tristan Loisel\n",
        "### Professor: Benjamin Muller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Iu-Peuehh1Rv",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import gzip\n",
        "from urllib.request import urlopen\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ktgxxr0yihb5"
      },
      "source": [
        "# Data import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn8t0NnN9RL3",
        "colab_type": "text"
      },
      "source": [
        "We import two datasets:\n",
        "\n",
        "\n",
        "*   one containing **all the reviews** and overall ratings of products categorized as video games by amazon\n",
        "*   the second one containing all **data about this products** (title, description, price...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gN9aVkUQikWW",
        "outputId": "b3286a8c-0f90-4033-f68c-3a2cfb6b3947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "! wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Video_Games.json.gz\n",
        "\n",
        "! wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Video_Games.json.gz\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-12 08:21:24--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Video_Games.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 522823613 (499M) [application/octet-stream]\n",
            "Saving to: ‘Video_Games.json.gz.1’\n",
            "\n",
            "Video_Games.json.gz  81%[===============>    ] 404.28M  10.2MB/s    eta 9s     "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OL9XKoWYig14",
        "colab": {}
      },
      "source": [
        "def get_data(file):\n",
        "  data = []\n",
        "  with gzip.open(file) as f:\n",
        "      for l in f:\n",
        "          data.append(json.loads(l.strip()))\n",
        "  return(pd.DataFrame.from_dict(data))\n",
        "\n",
        "reviews = get_data('Video_Games.json.gz')\n",
        "metadata = get_data('meta_Video_Games.json.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zj39Da5NjYfh"
      },
      "source": [
        "# Data cleaning \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3xuOTmyIjmrK"
      },
      "source": [
        "We first delete the columns we deemed useless in the metadata (i.e. data about the products)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "06MTJ0rsjavj",
        "colab": {}
      },
      "source": [
        "del metadata[\"also_buy\"]\n",
        "del metadata[\"also_view\"]\n",
        "del metadata[\"similar_item\"]\n",
        "del metadata[\"feature\"]\n",
        "del metadata[\"details\"]\n",
        "del metadata[\"tech1\"]\n",
        "del metadata[\"tech2\"]\n",
        "del metadata[\"main_cat\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YWXu-lN2jqHn"
      },
      "source": [
        "Then we delete NaNs in date and price columns, as we need them to be clean. The date in question is the launching date of the product on amazon.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V32zeMcSqpda",
        "colab": {}
      },
      "source": [
        "clean_metadata = metadata.dropna(subset=['date','price']) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_sw3n7tslyEW"
      },
      "source": [
        "And we only keep one line for each product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Di_it-JkFXZ",
        "colab": {}
      },
      "source": [
        "clean_metadata = clean_metadata.drop_duplicates(subset='asin', keep=\"last\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Br_tpGDFmLk0"
      },
      "source": [
        "We clean the prices in order to turn them into numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "unJjSB1MmNox",
        "colab": {}
      },
      "source": [
        "import re\n",
        "indexes = clean_metadata.index.values.tolist()\n",
        "clean_prices = np.empty(len(clean_metadata))\n",
        "clean_prices[:]= np.NaN\n",
        "j=0\n",
        "for i in indexes:\n",
        "  s = clean_metadata[\"price\"][i]\n",
        "\n",
        "  price = re.sub('\\D', '', s[:10])\n",
        "  clean_prices[j] = float(price)/100.0\n",
        "\n",
        "  j+=1\n",
        "\n",
        "clean_metadata.insert(7, \"clean_price\", clean_prices, True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxnW2g9rSykq",
        "colab_type": "text"
      },
      "source": [
        "We convert the dates to Unix time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o98UgKtsS2B8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "products_dates_unix = dict()\n",
        "max_date = 0\n",
        "date = \"\"\n",
        "errors =0\n",
        "for i in clean_metadata.index.values.tolist():\n",
        "  string_date = clean_metadata[\"date\"][i]\n",
        "  asin = clean_metadata[\"asin\"][i]\n",
        "  try:\n",
        "    products_dates_unix[asin] =time.mktime(datetime.datetime.strptime(string_date, '%B %d, %Y').timetuple())\n",
        "    if products_dates_unix[asin] > max_date:\n",
        "      products_dates_unix[asin] > max_date\n",
        "      date = string_date\n",
        "  except ValueError:\n",
        "    errors +=1\n",
        "print(\"Proportion of errors: \" + str(errors/len(clean_metadata)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZYCsk4LiH90P"
      },
      "source": [
        "We create a column containing, for each product, the number of 5/5 ratings it obtained in the four years following its release. It will enable us to create the variable we want to predict. \n",
        "\n",
        "N.B: we chose four years because the latest products in this data were released in 2016. We also tried two and three years but it led to a lower performance of the feature.\n",
        "\n",
        "The high proportion of KeyError comes from the fact that many reviews are the ones of products absent from our clean metadata because they had incomplete information (NaNs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gG3j-C9aG5uI",
        "colab": {}
      },
      "source": [
        "ratings = reviews[[\"asin\",\"overall\",\"reviewText\",\"unixReviewTime\"]]\n",
        "\n",
        "ratings_per_product = defaultdict(lambda:[])\n",
        "reviews_per_product = defaultdict(lambda:[])\n",
        "\n",
        "\n",
        "for i in tqdm(range(len(ratings))):\n",
        "  try:\n",
        "    id_object = ratings[\"asin\"][i]\n",
        "    rating_date = ratings[\"unixReviewTime\"][i]\n",
        "\n",
        "    product_date = products_dates_unix[id_object]\n",
        "    if product_date + 4*365*24*3600 > rating_date : #we only take the reviews which appaear less \n",
        "                                                    #than four years after the release of the product\n",
        "      ratings_per_product[id_object].append(ratings[\"overall\"][i])\n",
        "      reviews_per_product[id_object].append(ratings[\"reviewText\"][i])\n",
        "  except KeyError: # There are some errors due to the fact that metacleandata does not countain all products\n",
        "    ()\n",
        "\n",
        "\n",
        "number_fives = dict() # In this variable we write the number of overall ratings equal to five within the reviews\n",
        "\n",
        "for obj in ratings_per_product: \n",
        "  ratings_per_product[obj] = np.array(ratings_per_product[obj])\n",
        "  number_fives[obj] = np.count_nonzero(ratings_per_product[obj] - 5.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgFrZN49LVbc",
        "colab_type": "text"
      },
      "source": [
        "We then add the number of 5/5 ratings (in the four years after the release) as a column in our dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Hhj5tbhpiRG",
        "colab": {}
      },
      "source": [
        "nb_fives_column = np.empty(len(clean_metadata))\n",
        "nb_fives_column[:]= np.NaN\n",
        "j=0\n",
        "errors=0\n",
        "for i in indexes:\n",
        "    try:\n",
        "        nb_fives_column[j] = number_fives[clean_metadata[\"asin\"][i]]\n",
        "    except KeyError:\n",
        "        nb_fives_column[j]=0\n",
        "        errors +=1\n",
        "    j+=1\n",
        "print(\"Proportion of errors: \" + str(errors/len(indexes)))\n",
        "clean_metadata.insert(2, \"nb_fives\", nb_fives_column, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z8TUzLpAbInt"
      },
      "source": [
        "We split the data into two separate sets:\n",
        "\n",
        "- data_for_features will be used as an environment to compute some \"external\"/macro features (brand score and doc2vec nearest neighbors)\n",
        "\n",
        "- data_for_model on which we will train and test our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aOxP0tGqaJUG",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "seed = 10\n",
        "data_for_features,data_for_model,_,_ = train_test_split(clean_metadata, clean_metadata, test_size=int(len(clean_metadata)/2), random_state = seed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-RsF4VBTIQJk"
      },
      "source": [
        "# Creation of our features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wOU1LZTVsxHx",
        "colab": {}
      },
      "source": [
        "#this step is just a quick tokenization\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_uQKRTTFqpan"
      },
      "source": [
        "## Description/Title length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoGS44W6AHt0",
        "colab_type": "text"
      },
      "source": [
        "Our first features will simply be the length of a product's title (ie denomination) and description (given to Amazon by the manufacture)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ahICpk9492eF",
        "colab": {}
      },
      "source": [
        "def compute_length_feature(data):\n",
        "  titles = list(data[\"title\"])\n",
        "  descriptions = list(data[\"description\"])\n",
        "  length_title = np.zeros(len(data))\n",
        "  length_description = np.zeros(len(data))\n",
        "  for i in tqdm(range(len(data))):\n",
        "    length_title[i] = len(str(titles[i]))\n",
        "    length_description[i] = len(str(descriptions[i]))\n",
        "  return((length_title,length_description))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P6WVy7ur_BQl",
        "colab": {}
      },
      "source": [
        "X_title_length,X_description_length = compute_length_feature(data_for_model)\n",
        "# X_something designate an array containing the feature \"something\" for each product"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AqYAWKxAEiAE"
      },
      "source": [
        "## Availability of a product's picture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C-C2d2y5B79b"
      },
      "source": [
        "For a given product, this feature is worth 1 if a picture of this product is available on Amazon, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-paLmxbF_wku",
        "colab": {}
      },
      "source": [
        "X_images = np.zeros(len(data_for_model))\n",
        "images = list(data_for_model[\"image\"])\n",
        "for i in range(len(data_for_model)):\n",
        "  if str(images[i]) == \"nan\":\n",
        "    X_images[i]=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7_-W_PhvElLw"
      },
      "source": [
        "## Word2Vec similarity (descriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ODQoRycBMmv",
        "colab_type": "text"
      },
      "source": [
        "This feature works as follows:\n",
        "- we load a pre-trained word2vec model\n",
        "- we choose a reference word we identified as relevant in this context: \"garantee\", \"quality\", \"best\", \"delivery\" etc.\n",
        "- for a given description, we compute a score according to the similarity its words have with our chosen word\n",
        "\n",
        "We compute this feature for several key-words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlP_E2V9LxwP",
        "colab_type": "text"
      },
      "source": [
        "We load a pre-trained model of word2vec in order to computer some similarity feature from the descriptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIWD4at4ND3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.downloader as api\n",
        "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
        "print(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_mclyGyN7-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "\n",
        "# Load pretrained model \n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc-ExRKhOBGb",
        "colab_type": "text"
      },
      "source": [
        "Computed feature: highest word similarity between given words (\"guarantee\",\"best\"...) and the words of the description "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8nUlzSMFuud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def compute_similarity_features_descriptions(descriptions,ref_words):\n",
        "  similarity_features = np.array([[0.0 for i in range(len(ref_words))] for j in range(len(descriptions))])\n",
        "  stop_words = stopwords.words('english') \n",
        "  description_index=0\n",
        "  for d in tqdm(descriptions):\n",
        "    if type(d)==list:\n",
        "      d_list = word_tokenize(d[0]) #tokenization \n",
        "      d_list = [w for w in d_list if not w in stop_words]\n",
        "      ref_index=0\n",
        "      for ref_word in ref_words:\n",
        "        maxscore = 0\n",
        "        for w in d_list:\n",
        "          try:\n",
        "            score = word2vec_model.similarity(w,ref_word)/len(d_list) #similarity computation\n",
        "          except KeyError:\n",
        "            ()\n",
        "          maxscore = score if maxscore < score else maxscore\n",
        "        similarity_features[description_index,ref_index]+=maxscore\n",
        "        ref_index +=1\n",
        "    description_index+=1\n",
        "\n",
        "  return(similarity_features) #we normalize the output such that it is a real number in [0,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOegHTF4Pczw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_similarity_w2v = compute_similarity_features_descriptions(data_for_model[\"description\"],[\"guarantee\",\"quality\",\"best\",\"new\",\"delivery\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RuPMpmSmwXT",
        "colab_type": "text"
      },
      "source": [
        "## Word2vec similarity (for reviews, not used in the final model but efficient)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAPaQ6tbMRHW",
        "colab_type": "text"
      },
      "source": [
        "Same feature as the one computed for the descriptions, except that there are several reviews per product (whereas we only have one description). Therefore, we decided to average the highest word similarity across the reviews.\n",
        "\n",
        "Also, it is important to notice that this feature serves a slightly different purpose that our main one. \n",
        "\n",
        "Our main purpose is to predict, at a product's launching time, how much success it is likely to have. For this main purpose, the reviews are irrelevant because they come from \"the future\".\n",
        "\n",
        "However, looking at the reviews allows to study the predictability of a product's score given its characteritics and reviews. Given the little supplementary works it required to adapt our former feature to reviews, we thought it was worth to do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4k3pRKymwXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def compute_similarity_features_reviews(id_products,ref_words):\n",
        "#     similarity_features = np.array([[0.0 for i in range(len(ref_words))] for j in range(len(id_products))])\n",
        "#     stop_words = stopwords.words('english') \n",
        "#     data_index=0\n",
        "#     for i in tqdm(range(len(id_products))):\n",
        "#     #tokenization \n",
        "#         id_object = id_products[i]\n",
        "#         reviews_list = reviews_per_product[id_object][:5]\n",
        "#         for r in reviews_list:\n",
        "#             if type(r)==str:\n",
        "#                 r_list = word_tokenize(r)\n",
        "#                 r_list = [w for w in r_list if not w in stop_words]\n",
        "#                 ref_index=0\n",
        "#                 for ref_word in ref_words:\n",
        "#                     for w in r_list:\n",
        "#                         maxscore = 0\n",
        "#                         try:\n",
        "#                           #similarity\n",
        "#                           score = word2vec_model.similarity(w,ref_word)/(len(r_list)*len(reviews_list))\n",
        "#                         except KeyError:\n",
        "#                           ()\n",
        "#                         maxscore = score if maxscore < score else maxscore\n",
        "#                     similarity_features[data_index,ref_index]+=maxscore\n",
        "#                     ref_index +=1\n",
        "#         data_index+=1\n",
        "\n",
        "#     return(np.array(similarity_features)) #we normalize the output such that it is a real number in [0,1]\n",
        "        \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WJ2ynbsVyCQn",
        "colab": {}
      },
      "source": [
        "# X_similarity_reviews_w2v = compute_similarity_features_reviews(list(data_for_model[\"asin\"]),[\"guarantee\",\"quality\",\"best\",\"awesome\",\"game\",\"new\",\"essential\",\"delivery\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FMDcUQRznBch"
      },
      "source": [
        "## Brand score (computed on data_for_features to avoid overfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVe4lQFlMtg5",
        "colab_type": "text"
      },
      "source": [
        "We have some information about the product's brands. This feature is the average number of 5/5 ratings obtained by all the products of data_for_features within a given brand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K4kzkG1Ld7kh",
        "colab": {}
      },
      "source": [
        "brands = set(list(clean_metadata[\"brand\"]))\n",
        "\n",
        "brand_scores = dict()\n",
        "for b in brands:\n",
        "  brand_scores[b]=[]\n",
        "data_for_features_indexes = data_for_features.index.values.tolist()\n",
        "\n",
        "for i in data_for_features_indexes:\n",
        "  try:\n",
        "    brand_scores[data_for_features[\"brand\"][i]].append(data_for_features[\"nb_fives\"][i])\n",
        "    \n",
        "  except KeyError:\n",
        "    ()\n",
        "for b in brands:\n",
        "  brand_scores[b]=np.mean(np.array(brand_scores[b]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "doMDc5bxofKU",
        "colab": {}
      },
      "source": [
        "def compute_brand_feature(data):\n",
        "  brand_feature = np.zeros(len(data))\n",
        "  brands_list = list(data[\"brand\"])\n",
        "  i=0\n",
        "  for b in tqdm(brands_list):\n",
        "    brand_feature[i]=brand_scores[b]\n",
        "    i+=1\n",
        "  return(brand_feature)\n",
        "\n",
        "X_brand_score = compute_brand_feature(data_for_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "inZ6nVqsE0Gx"
      },
      "source": [
        "## Nearest neighbors with Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4guumBPzEFiC",
        "colab_type": "text"
      },
      "source": [
        "In this part, we train a doc2vec model on the set of descriptions.\n",
        "\n",
        "Note that we made the choice not to suppress from our dataset the products with an empty description field. Instead, we separate them from the the others products of data_for_features and just give them the value -1 as a feature.\n",
        "\n",
        "Then, we train a doc2vec model on the no-empty descriptions from data_for_features. We thus obtain a representation of our descriptions as vectors. We then take, for a given description, the 10 nearest description and the sum of the associated product's score. By score, we mean our objective: 1 if a given product has enouhg 5/5 ratings and 0 otherwise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AD49mBUE32l",
        "colab_type": "text"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sGGHzDdq6OTH",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from nltk import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models.doc2vec import Doc2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tN6aY8tkE3ts"
      },
      "source": [
        "Data selection before training (we delete NaNs) to get a sub-dataset of products with non-empty descriptions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4JASCTPQ6PTy",
        "colab": {}
      },
      "source": [
        "doc2vec_indexes_to_drop = np.array([i for i in data_for_features.index.values.tolist() if type(data_for_features[\"description\"][i])!=list])\n",
        "doc2vec_train_data = data_for_features.drop(doc2vec_indexes_to_drop)\n",
        "\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(str(_d).lower()), tags=[str(i)]) for i, _d in enumerate(doc2vec_train_data[\"description\"])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GMsMMmDQFBE4"
      },
      "source": [
        "Doc2Vec model training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aBrMJwaM_xnB",
        "colab": {}
      },
      "source": [
        "max_epochs = 10\n",
        "vec_size = 20\n",
        "alpha = 0.025\n",
        "\n",
        "doc2vec_model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "doc2vec_model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    doc2vec_model.train(tagged_data,\n",
        "                total_examples=doc2vec_model.corpus_count,\n",
        "                epochs=doc2vec_model.iter)\n",
        "    # decrease the learning rate\n",
        "    doc2vec_model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    doc2vec_model.min_alpha = doc2vec_model.alpha\n",
        "\n",
        "doc2vec_model.save(\"d2v.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PmeSWjfOFDzM"
      },
      "source": [
        "Computation of the product scores in our sub-dataset. These scores are deterministic and will be needed to compute, for a given description, the scores corresponding to its 10 nearest neighbors in our doc2vec model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QaptR-KZBh-o",
        "colab": {}
      },
      "source": [
        "labels =  np.array(list(doc2vec_train_data[\"nb_fives\"]))\n",
        "for i in range(len(labels)):\n",
        "  if labels[i]>2:\n",
        "    labels[i]=1\n",
        "  else:\n",
        "    labels[i]=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qzEYT6miFewI"
      },
      "source": [
        "Finally, we compute the feature: for a given product: \n",
        "\n",
        "1.   if this product's description is empty, the feature is worth -1\n",
        "2.   else, we take the 10 products the descriptions of which are the nearest of its description and we sum these products scores to get our feature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ICCIyCRb_15U",
        "colab": {}
      },
      "source": [
        "def compute_doc2vec_feature_description(data):\n",
        "  \n",
        "  doc2vec_feature = np.zeros(len(data))\n",
        "  j=0\n",
        "  \n",
        "  for idx in tqdm(data.index.values.tolist()):\n",
        "    description_i = data[\"description\"][idx]\n",
        "    if not type(description_i)==list:\n",
        "      doc2vec_feature[j] = -1\n",
        "    else:\n",
        "      test_data = word_tokenize(str(description_i).lower())\n",
        "      v_idx = doc2vec_model.infer_vector(test_data)\n",
        "      nearest_neighbors = [int(tup[0]) for tup in doc2vec_model.docvecs.most_similar([v_idx],topn = 10 )]\n",
        "      doc2vec_feature[j] = labels[nearest_neighbors].sum()\n",
        "    j+=1\n",
        "  return(doc2vec_feature)\n",
        "\n",
        "doc2vec_feature_descriptions = compute_doc2vec_feature_description(data_for_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Z_OlDSIl_Cj"
      },
      "source": [
        "Same feature with the product title (ie denomination)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VJQZNYN3mDYo",
        "colab": {}
      },
      "source": [
        "tagged_data = [TaggedDocument(words=word_tokenize(str(_d).lower()), tags=[str(i)]) for i, _d in enumerate(data_for_features[\"title\"])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c7r0pbmbmTAQ",
        "colab": {}
      },
      "source": [
        "max_epochs = 10\n",
        "vec_size = 20\n",
        "alpha = 0.025\n",
        "\n",
        "doc2vec_model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "doc2vec_model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    doc2vec_model.train(tagged_data,\n",
        "                total_examples=doc2vec_model.corpus_count,\n",
        "                epochs=doc2vec_model.iter)\n",
        "    # decrease the learning rate\n",
        "    doc2vec_model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    doc2vec_model.min_alpha = doc2vec_model.alpha\n",
        "\n",
        "doc2vec_model.save(\"d2v.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ArfADT2BmX6Y",
        "colab": {}
      },
      "source": [
        "labels =  np.array(list(data_for_features[\"nb_fives\"]))\n",
        "for i in range(len(labels)):\n",
        "  if labels[i]>2:\n",
        "    labels[i]=1\n",
        "  else:\n",
        "    labels[i]=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4jk2H1QHl7eO",
        "colab": {}
      },
      "source": [
        "def compute_doc2vec_feature_title(data):\n",
        "\n",
        "  doc2vec_feature = np.zeros(len(data))\n",
        "  j=0\n",
        "  for idx in tqdm(data.index.values.tolist()):\n",
        "\n",
        "    title_i = data[\"title\"][idx]\n",
        "\n",
        "    if not type(title_i)==str:\n",
        "      doc2vec_feature[j] = -1\n",
        "    else:\n",
        "      test_data = word_tokenize(str(title_i).lower())\n",
        "      v_idx = doc2vec_model.infer_vector(test_data)\n",
        "      nearest_neighbors = [int(tup[0]) for tup in doc2vec_model.docvecs.most_similar([v_idx],topn = 10 )]\n",
        "      doc2vec_feature[j] = labels[nearest_neighbors].sum()\n",
        "    j+=1\n",
        "  return(doc2vec_feature)\n",
        "\n",
        "doc2vec_feature_titles = compute_doc2vec_feature_title(data_for_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x4g8GhTLtPI",
        "colab_type": "text"
      },
      "source": [
        "## Price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgqQieLTLvb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_price = data_for_model[\"clean_price\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SXjCD4U1T1Fk"
      },
      "source": [
        "# Design of a prediction model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnHKKjg4Ht1L",
        "colab_type": "text"
      },
      "source": [
        "We use an XGBoost prediction model. We train it on data_for_features and test it on data_for_model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NBMKtBtILSY",
        "colab_type": "text"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IytC-LXhUGaG",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOPPr14HIVhk",
        "colab_type": "text"
      },
      "source": [
        "We transpose our similarity estimators for technical considerations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIsuQMAoQ_jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_similarity_w2v = np.transpose(X_similarity_w2v)\n",
        "#X_similarity_reviews_w2v = np.transpose(X_similarity_reviews_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX9SxIL9PY7R",
        "colab_type": "text"
      },
      "source": [
        "## Feature selection:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7aLSwbuIiC5",
        "colab_type": "text"
      },
      "source": [
        "Below, the features with a \"True\" value are going to be tested for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFWk7q6nNXG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = {\"Image\":True,\n",
        "            \"doc2vec_nearest_neighbors_titles\":True,\n",
        "            \"doc2vec_nearest_neighbors_descriptions\":True,\n",
        "            \"Title_length\":True,\n",
        "            \"Description_length\":True,\n",
        "            \"Brand_score\":True,\n",
        "            \"Price\":True,\n",
        "            \"word2vec_similarity_to_guarantee\":True,\n",
        "            \"word2vec_similarity_to_quality\":True,\n",
        "            \"word2vec_similarity_to_best\":True,\n",
        "            \"word2vec_similarity_to_new\":True,\n",
        "            \"word2vec_similarity_to_delivery\":True\n",
        "\n",
        "}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPOy1YBmInqf",
        "colab_type": "text"
      },
      "source": [
        "In what folows, X_all contains every feature. X contains only the features we selected above. Below, do some technical preparatives and set the size of our testing set relatively to our dataset size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ0nXG1lRkeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_all = np.vstack((X_images, doc2vec_feature_titles,doc2vec_feature_descriptions,X_title_length,X_description_length,X_brand_score,X_price))\n",
        "\n",
        "X_all = np.vstack((X_all, X_similarity_w2v))\n",
        "#X_all = np.vstack((X_all, X_similarity_reviews_w2v))\n",
        "X_all = np.transpose(X_all)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SjLvQotdUO4d",
        "colab": {}
      },
      "source": [
        "\n",
        "all_columns = [\"Image\",\"doc2vec_nearest_neighbors_titles\",\n",
        "\"doc2vec_nearest_neighbors_descriptions\",\"Title_length\",\n",
        "\"Description_length\",\"Brand_score\",\"Price\",\n",
        "\"word2vec_similarity_to_guarantee\",\n",
        "\"word2vec_similarity_to_quality\",\n",
        "\"word2vec_similarity_to_best\",\n",
        "\"word2vec_similarity_to_new\",\n",
        "\"word2vec_similarity_to_delivery\"]\n",
        "\n",
        "X=np.empty(len(data_for_model))\n",
        "columns = []\n",
        "X_empty = True\n",
        "i=0\n",
        "for c in all_columns:\n",
        "  if features[c]:\n",
        "    columns.append(c)\n",
        "    if X_empty:\n",
        "      X = X_all[:,i]\n",
        "      X_empty = False\n",
        "    else:\n",
        "      X = np.vstack((X,X_all[:,i]))\n",
        "      \n",
        "  i+=1\n",
        "X=X.transpose()\n",
        "X = pd.DataFrame(data = X,columns = columns )\n",
        "\n",
        "Y = np.array(list(data_for_model[\"nb_fives\"]))\n",
        "for i in range(len(Y)):\n",
        "  if Y[i]>2:\n",
        "    Y[i]=1\n",
        "  else:\n",
        "    Y[i]=0\n",
        "\n",
        "\n",
        "test_size = 0.3\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state = 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-P1MDeQJ-ud",
        "colab_type": "text"
      },
      "source": [
        "In the next cell, we compute the proportion of products whose score is 1. It gives in fact the correction rate of the constant estimator which always predict 1. It will be a good reference to assert the quality of our estimator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5WRxNnu3Y1Ru",
        "colab": {}
      },
      "source": [
        "# Proportion of labels equal to 1\n",
        "np.sum(Y)/len(Y)\n",
        "\n",
        "# This proportion must be close to 0.50, else the best trivial predictor \n",
        "# (either the one always equal to 0 or the one always equal to 1) would be difficult to outperform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lob3gieI1Vg",
        "colab_type": "text"
      },
      "source": [
        "# Definition, training and prediction of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bo0JqBFoW78r",
        "colab": {}
      },
      "source": [
        "# fit model to training data\n",
        "model = XGBClassifier(max_depth = 3, learning_rate = 0.1,n_estimators=100, objective = 'binary:logistic')\n",
        "\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# make predictions for test data\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "f1_sc = f1_score(Y_test, Y_pred)\n",
        "print(\"f1-score: %.2f%%\" % (f1_sc*100.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "andISZWkI7_R",
        "colab_type": "text"
      },
      "source": [
        "# Features importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9sU0gptFOGsG",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb.plot_importance(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjmqeKt2qBVf",
        "colab_type": "text"
      },
      "source": [
        "We are surprised the presence of a picture is the least important feature. We thus check how many picture have a picture. We find that about 17% of the procucts have a picture, meaning that the feature concerns a minority of products and thus have little importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnWmAwTSp0SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.sum(X_images)/len(X_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc2fRsvpI_nP",
        "colab_type": "text"
      },
      "source": [
        "# Graphic representation of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnj04d8ImwXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb.to_graphviz(model, num_trees=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bMnNv_cjhVpq"
      },
      "source": [
        "# Result of the prediction model ***using only textual features***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dtizbgw5hVsf"
      },
      "source": [
        "## Feature selection: we keep only textual features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J7R8vBo3hVsk"
      },
      "source": [
        "Below, the features with a \"True\" value are going to be tested for our model.\n",
        "We give a \"False\" value to all features using the reviews because we remind the reader that our main objective is to build an estimator predicting a product's success only from the information avalaible before its launching time, of which reviews are not a part of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IcMnOCP4hVsp",
        "colab": {}
      },
      "source": [
        "features = {\"Image\":False,\n",
        "            \"doc2vec_nearest_neighbors_titles\":True,\n",
        "            \"doc2vec_nearest_neighbors_descriptions\":True,\n",
        "            \"Title_length\":False,\n",
        "            \"Description_length\":False,\n",
        "            \"Brand_score\":False,\n",
        "            \"Price\":False,\n",
        "            \"word2vec_similarity_to_guarantee\":True,\n",
        "            \"word2vec_similarity_to_quality\":True,\n",
        "            \"word2vec_similarity_to_best\":True,\n",
        "            \"word2vec_similarity_to_new\":True,\n",
        "            \"word2vec_similarity_to_delivery\":True\n",
        "\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QjmjrdEBhVtF"
      },
      "source": [
        "In what folows, X_all contains every feature. X contains only the features we selected above. Below, do some technical preparatives and set the size of our testing set relatively to our dataset size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nSQ06VXjhVtJ",
        "colab": {}
      },
      "source": [
        "X_all = np.vstack((X_images, doc2vec_feature_titles,doc2vec_feature_descriptions,X_title_length,X_description_length,X_brand_score,X_price))\n",
        "\n",
        "X_all = np.vstack((X_all, X_similarity_w2v))\n",
        "X_all = np.transpose(X_all)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3MG7zJLhVte",
        "colab": {}
      },
      "source": [
        "\n",
        "all_columns = [\"Image\",\"doc2vec_nearest_neighbors_titles\",\n",
        "\"doc2vec_nearest_neighbors_descriptions\",\"Title_length\",\n",
        "\"Description_length\",\"Brand_score\",\"Price\",\n",
        "\"word2vec_similarity_to_guarantee\",\n",
        "\"word2vec_similarity_to_quality\",\n",
        "\"word2vec_similarity_to_best\",\n",
        "\"word2vec_similarity_to_new\",\n",
        "\"word2vec_similarity_to_delivery\"]\n",
        "\n",
        "X=np.empty(len(data_for_model))\n",
        "columns = []\n",
        "X_empty = True\n",
        "i=0\n",
        "for c in all_columns:\n",
        "  if features[c]:\n",
        "    columns.append(c)\n",
        "    if X_empty:\n",
        "      X = X_all[:,i]\n",
        "      X_empty = False\n",
        "    else:\n",
        "      X = np.vstack((X,X_all[:,i]))\n",
        "      \n",
        "  i+=1\n",
        "X=X.transpose()\n",
        "X = pd.DataFrame(data = X,columns = columns )\n",
        "\n",
        "Y = np.array(list(data_for_model[\"nb_fives\"]))\n",
        "for i in range(len(Y)):\n",
        "  if Y[i]>2:\n",
        "    Y[i]=1\n",
        "  else:\n",
        "    Y[i]=0\n",
        "\n",
        "\n",
        "test_size = 0.3\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state = 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jg_Mr1VghVtx"
      },
      "source": [
        "In the next cell, we compute the proportion of products whose score is 0. It gives in fact the correction rate of the constant estimator which always predict 0. It will be a good reference to assert the quality of our estimator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6QcV9UmnhVt0",
        "colab": {}
      },
      "source": [
        "# Proportion of labels equal to 1\n",
        "np.sum(Y)/len(Y)\n",
        "\n",
        "# This proportion must be close to 0.50, else the best trivial predictor \n",
        "# (either the one always equal to 0 or the one always equal to 1) would be difficult to outperform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PyIwNd_jhVuD"
      },
      "source": [
        "# Definition, training and prediction of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hkq7nbo1hVuE",
        "colab": {}
      },
      "source": [
        "# fit model to training data\n",
        "textual_model = XGBClassifier(max_depth = 3, learning_rate = 0.1,n_estimators=100, objective = 'binary:logistic')\n",
        "\n",
        "textual_model.fit(X_train, Y_train)\n",
        "\n",
        "# make predictions for test data\n",
        "Y_pred = textual_model.predict(X_test)\n",
        "\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "f1_sc = f1_score(Y_test, Y_pred)\n",
        "print(\"f1-score: %.2f%%\" % (f1_sc*100.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anHb0f3W7R2Y",
        "colab_type": "text"
      },
      "source": [
        "# A greedy algorithm for feature selection\n",
        "\n",
        "Here we try to find the best features with a simple algorithm:\n",
        "\n",
        "We start with a feature we identified as always performant which is the brand score. From a qualitative point of view, it seems relevant that the brand is part of our greedy estimator because it gives an information intuitively relevant and independant from the others features. For each other feature, we train and evaluate the model, and we keep the one feature which gives the best accuracy. We now have two features and start again this procedure, until the accuracy does not increase any more. It must be run several times, in order not to overfit the data.\n",
        "\n",
        "We used this algorithm to select the features present in this notebook. The features we rejected were for instance word2vec similarity features with words such as \"essential\", \"game\"...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC8onMqw77Mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_features = 12\n",
        "T = [False for i in range(nb_features)] # Table containing True when the corresponding feature is selected, else False\n",
        "T[0]=True # We know that the brand score is our best feature\n",
        "best_accuracy = 0\n",
        "improvement = True\n",
        "while improvement: #while the accuracy can be improved\n",
        "  features_to_try = [i for i in range(nb_features) if not T[i]]\n",
        "  best_feature = 0\n",
        "  best_accuracy_temp = 0\n",
        "  for i in features_to_try :\n",
        "    T[i]=True\n",
        "\n",
        "    features = {\"Brand_score\":T[0],\n",
        "            \"Image\":T[1],\n",
        "            \"doc2vec_nearest_neighbors_titles\":T[2],\n",
        "            \"doc2vec_nearest_neighbors_descriptions\":T[3],\n",
        "            \"Title_length\":T[4],\n",
        "            \"Description_length\":T[5],\n",
        "            \"Price\":T[6],\n",
        "            \"word2vec_similarity_to_guarantee\":T[7],\n",
        "            \"word2vec_similarity_to_quality\":T[8],\n",
        "            \"word2vec_similarity_to_best\":T[9],\n",
        "            \"word2vec_similarity_to_new\":T[10],\n",
        "            \"word2vec_similarity_to_delivery\":T[11]\n",
        "\n",
        "      }\n",
        "\n",
        "    X=np.empty(len(data_for_model))\n",
        "    columns = []\n",
        "    X_empty = True\n",
        "    j=0\n",
        "    for c in all_columns:\n",
        "      if features[c]:\n",
        "        columns.append(c)\n",
        "        if X_empty:\n",
        "          X = X_all[:,j]\n",
        "          X_empty = False\n",
        "        else:\n",
        "          X = np.vstack((X,X_all[:,j]))     \n",
        "      j+=1\n",
        "    X=X.transpose()\n",
        "    X = pd.DataFrame(data = X,columns = columns)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n",
        "    model = XGBClassifier(max_depth = 3, learning_rate = 0.1,n_estimators=100, objective = 'binary:logistic')\n",
        "\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    # make predictions for test data\n",
        "    Y_pred = model.predict(X_test)\n",
        "\n",
        "    # evaluate predictions\n",
        "    accuracy_temp = accuracy_score(Y_test, Y_pred)\n",
        "    if best_accuracy_temp < accuracy_temp:\n",
        "      best_accuracy_temp = accuracy_temp\n",
        "      best_feature = i\n",
        "    T[i]=False\n",
        "  if best_accuracy_temp > best_accuracy:\n",
        "    best_accuracy = best_accuracy_temp\n",
        "  else: \n",
        "    improvement = False\n",
        "  T[best_feature]=True\n",
        "  accuracy = best_accuracy_temp\n",
        "  print(\"Selected feature: \" + str(best_feature))\n",
        "  print(\"accuracy = \" + str(best_accuracy))\n",
        "\n",
        "best_T = T\n",
        "print(\"The best selection of features is given by this table: \" + str(best_T))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSCKP5cwUPBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_T # The best features are given by this table"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
